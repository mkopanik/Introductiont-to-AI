{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1\n",
    "\n",
    "1. Generate strings of length 15 over the alphabet a, b, c, d\n",
    "\n",
    "2. Label your strings basing on matching a 5-element regular expression\n",
    "\n",
    "3. Balance your dataset of size 10000 so that approximately half of the dataset contains regex-matching parts.\n",
    "\n",
    "4. Prepare your data for training using one-hot encoding\n",
    "\n",
    "5. Divide your dataset into training and testing parts.\n",
    "\n",
    "6. Implement and train a model consisting of one convolutional layer with one filter followed by one fully-connected layer and train it to classify your strings. After training, examine the values of the filter\n",
    "\n",
    "7. Implement and train more complex models (more filters, layers) and analyze their performance on the prepared dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "def generate_string(length = 15, alphabet = \"abcd\", pattern = \"cabad\", contains = True):\n",
    "      \n",
    "      pattern_len = len(pattern)\n",
    "      if contains: \n",
    "            # we need to generate prefix and sufix to our pattern both might be 0 lentgth ofc \n",
    "            prefix_len = random.randint(0,length - pattern_len)\n",
    "            suffix_len = length - pattern_len - prefix_len\n",
    "            prefix = ''.join(random.choices(\"abcd\" , k=prefix_len))\n",
    "            suffix = ''.join(random.choices(\"abcd\" , k=suffix_len))\n",
    "            return prefix + pattern + suffix\n",
    "      else: \n",
    "            while True:\n",
    "                  candidate = ''.join(random.choices(\"abcd\" , k=length))\n",
    "                  if pattern not in candidate:\n",
    "                        return candidate\n",
    "def generate_dataset(size = 10000):\n",
    "      expressions = []\n",
    "      labels = []\n",
    "      half = size // 2\n",
    "      \n",
    "      \n",
    "      for _ in range(half):\n",
    "            expressions.append(generate_string(length=15,alphabet=\"abcd\",pattern=\"cabad\",contains=True))\n",
    "            labels.append(1)\n",
    "      for _ in range(size- half):\n",
    "            expressions.append(generate_string(length=15,alphabet=\"abcd\",pattern=\"cabad\",contains=False))\n",
    "            labels.append(0)\n",
    "            \n",
    "      return pd.DataFrame({\"expressions\":expressions,\"contains_abcd\":labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bdcbccdaddbdcdb'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_string(contains=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expressions</th>\n",
       "      <th>contains_abcd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cabadbcccadcccc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaacbaccabadcda</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bdacabadbccabba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bcbccbbcabadacb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cabadadbabddaba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>ccadaaadbabddcb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>ccbbdddcdcddcdc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>dcdcacbcbdcbdad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>bbabddaadbdcbdc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>adcbcbaabcccbcb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          expressions  contains_abcd\n",
       "0     cabadbcccadcccc              1\n",
       "1     aaacbaccabadcda              1\n",
       "2     bdacabadbccabba              1\n",
       "3     bcbccbbcabadacb              1\n",
       "4     cabadadbabddaba              1\n",
       "...               ...            ...\n",
       "9995  ccadaaadbabddcb              0\n",
       "9996  ccbbdddcdcddcdc              0\n",
       "9997  dcdcacbcbdcbdad              0\n",
       "9998  bbabddaadbdcbdc              0\n",
       "9999  adcbcbaabcccbcb              0\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indexes = {'a':0,'b':1,'c':2,'d':3}\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_expression(s,map,num_classes):\n",
    "    one_hot = torch.zeros(size = (len(s),num_classes))\n",
    "    for i, char in enumerate(s):\n",
    "        one_hot[i,map[char]] = 1 \n",
    "    return one_hot\n",
    "def one_hot_encode_dataset(dataset,shape,map):\n",
    "    table = torch.empty(size = shape)\n",
    "    for i,s in enumerate(dataset.iloc[:,0]):\n",
    "        table[i] = one_hot_encode_expression(s,map = map,num_classes= shape[-1])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_character(char,characters = ['a','b','c','d']):\n",
    "    index = torch.argmax(char)\n",
    "    return characters[index]\n",
    "\n",
    "def decode_string(dataset, index):\n",
    "    expression = \"\"\n",
    "    for letter in dataset[index]:\n",
    "        expression += decode_character(letter)\n",
    "    return expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = generate_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encode_expression(s,char_indexes,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.empty(size = (10000,15,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_dataset(dataset,shape,map):\n",
    "    table = torch.empty(size = shape)\n",
    "    for i,s in enumerate(dataset.iloc[:,0]):\n",
    "        table[i] = one_hot_encode_expression(s,map = map,num_classes= shape[-1])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_hot_encode_dataset(dataset=df,shape = (10000,15,4),map =char_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_string(dataset=X,index=5000) == df.iloc[5000,0]  # to make sure that one hot encoding works correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(data=df.iloc[:,1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X.view(10000,4,15),y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create simple conv model \n",
    "class SimpleConvNet(nn.Module):\n",
    "    # in channels is equal to number of alphabet letters from which dataset is constructed 4 in case of abcd \n",
    "    # 1 out channels = one filter \n",
    "    # let's go with kernel size equal to pattern length 5 in oour case\n",
    "    def __init__(self,num_chars,pattern_len):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=num_chars , out_channels = 1,kernel_size= pattern_len,bias = False),\n",
    "                                   nn.ReLU())\n",
    "        self.fc = nn.Linear(in_features=11,out_features=1,bias= False)\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConvNet(num_chars=4,pattern_len=5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000, 4, 15])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create custom dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    \n",
    "# Create Dataset instances\n",
    "train_dataset = TensorDataset(X_train, y_train.unsqueeze(1).type(torch.float32))\n",
    "test_dataset = TensorDataset(X_test, y_test.unsqueeze(1).type(torch.float32))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          dataloader,\n",
    "          loss_fn,\n",
    "          optimizer,\n",
    "          epochs = 10):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    # put model in train mode\n",
    "    \n",
    "\n",
    "      \n",
    "      # setrup train loss and accuracy \n",
    "      model.train()\n",
    "      train_loss , train_acc = 0,0\n",
    "      # loop through dataloader in batches \n",
    "      for batch, (X,y) in enumerate(dataloader):\n",
    "            \n",
    "\n",
    "            # forward pass\n",
    "            y_pred = model(X)\n",
    "          \n",
    "            # calcualte loss \n",
    "            loss = loss_fn(y_pred,y)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # zero grad\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # loss backward \n",
    "            loss.backward()\n",
    "            \n",
    "            # optimizer step \n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "           \n",
    "            # Adjust metrics to get average loss  per batch \n",
    "      train_loss = train_loss / len(dataloader)\n",
    "      \n",
    "      print(f\"Epoch{epoch} | train loss {train_loss}\")\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0939], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X[0].view(1,4,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0 | train loss 0.6555286486074329\n",
      "Epoch1 | train loss 0.6057020931318402\n",
      "Epoch2 | train loss 0.5914146407693625\n",
      "Epoch3 | train loss 0.5863809405267238\n",
      "Epoch4 | train loss 0.5830105316638946\n",
      "Epoch5 | train loss 0.5803992609679699\n",
      "Epoch6 | train loss 0.5781848866119981\n",
      "Epoch7 | train loss 0.5758644055202603\n",
      "Epoch8 | train loss 0.5748455246351659\n",
      "Epoch9 | train loss 0.5737670684792101\n",
      "Epoch10 | train loss 0.5732675383239985\n",
      "Epoch11 | train loss 0.5726861315779388\n",
      "Epoch12 | train loss 0.5718848325684667\n",
      "Epoch13 | train loss 0.5714852344617247\n",
      "Epoch14 | train loss 0.5711467188596725\n",
      "Epoch15 | train loss 0.570984202362597\n",
      "Epoch16 | train loss 0.5704294786415994\n",
      "Epoch17 | train loss 0.5700107009522617\n",
      "Epoch18 | train loss 0.5697064151987433\n",
      "Epoch19 | train loss 0.5693564495816826\n",
      "Epoch20 | train loss 0.5694944696873426\n",
      "Epoch21 | train loss 0.5692130570299924\n",
      "Epoch22 | train loss 0.5691816648468375\n",
      "Epoch23 | train loss 0.5689776170626283\n",
      "Epoch24 | train loss 0.5689629244804383\n",
      "Epoch25 | train loss 0.5688337470777333\n",
      "Epoch26 | train loss 0.5689328815788031\n",
      "Epoch27 | train loss 0.5688920753076673\n",
      "Epoch28 | train loss 0.5688095219992101\n",
      "Epoch29 | train loss 0.5687766115367413\n",
      "Epoch30 | train loss 0.5687380977347494\n",
      "Epoch31 | train loss 0.5688222756236792\n",
      "Epoch32 | train loss 0.5687271731346846\n",
      "Epoch33 | train loss 0.5687659640796483\n",
      "Epoch34 | train loss 0.5687938421964646\n",
      "Epoch35 | train loss 0.5687310083769261\n",
      "Epoch36 | train loss 0.5687176732160151\n",
      "Epoch37 | train loss 0.5687056188844144\n",
      "Epoch38 | train loss 0.5686461445130407\n",
      "Epoch39 | train loss 0.5687554595060647\n",
      "Epoch40 | train loss 0.5687216094695031\n",
      "Epoch41 | train loss 0.568832767419517\n",
      "Epoch42 | train loss 0.5688869634643197\n",
      "Epoch43 | train loss 0.568675571270287\n",
      "Epoch44 | train loss 0.568865776360035\n",
      "Epoch45 | train loss 0.5689406749233603\n",
      "Epoch46 | train loss 0.5688253564201295\n",
      "Epoch47 | train loss 0.5686036196164787\n",
      "Epoch48 | train loss 0.5688878276757896\n",
      "Epoch49 | train loss 0.5688055178150535\n",
      "Epoch50 | train loss 0.5686002166196704\n",
      "Epoch51 | train loss 0.5689307297579944\n",
      "Epoch52 | train loss 0.5688914189860225\n",
      "Epoch53 | train loss 0.5688293742388487\n",
      "Epoch54 | train loss 0.5684852637723088\n",
      "Epoch55 | train loss 0.5688608046248556\n",
      "Epoch56 | train loss 0.5687153788097202\n",
      "Epoch57 | train loss 0.5688423478603363\n",
      "Epoch58 | train loss 0.5686840589903295\n",
      "Epoch59 | train loss 0.5689181623235345\n",
      "Epoch60 | train loss 0.5687027706950903\n",
      "Epoch61 | train loss 0.5686322317272424\n",
      "Epoch62 | train loss 0.5687406262569129\n",
      "Epoch63 | train loss 0.568673991151154\n",
      "Epoch64 | train loss 0.5688409093208611\n",
      "Epoch65 | train loss 0.5688413172401487\n",
      "Epoch66 | train loss 0.5684711715765297\n",
      "Epoch67 | train loss 0.5687920027598738\n",
      "Epoch68 | train loss 0.5684650635905564\n",
      "Epoch69 | train loss 0.5688017546944321\n",
      "Epoch70 | train loss 0.5689798549190164\n",
      "Epoch71 | train loss 0.5686419700086117\n",
      "Epoch72 | train loss 0.5688655429333448\n",
      "Epoch73 | train loss 0.5688974164612591\n",
      "Epoch74 | train loss 0.5687158050388098\n",
      "Epoch75 | train loss 0.568651402387768\n",
      "Epoch76 | train loss 0.5687084010243416\n",
      "Epoch77 | train loss 0.5687619798444211\n",
      "Epoch78 | train loss 0.568770299591124\n",
      "Epoch79 | train loss 0.5685292940959334\n",
      "Epoch80 | train loss 0.5687152381427586\n",
      "Epoch81 | train loss 0.568452361356467\n",
      "Epoch82 | train loss 0.5687384767271578\n",
      "Epoch83 | train loss 0.5688930317759514\n",
      "Epoch84 | train loss 0.5686929077468812\n",
      "Epoch85 | train loss 0.5687324044853449\n",
      "Epoch86 | train loss 0.5689041367918253\n",
      "Epoch87 | train loss 0.5685525036975742\n",
      "Epoch88 | train loss 0.5688257328420877\n",
      "Epoch89 | train loss 0.5687453093938529\n",
      "Epoch90 | train loss 0.5687356212176382\n",
      "Epoch91 | train loss 0.5687563763931394\n",
      "Epoch92 | train loss 0.5686572143621743\n",
      "Epoch93 | train loss 0.5688195414841175\n",
      "Epoch94 | train loss 0.5687324820831418\n",
      "Epoch95 | train loss 0.5686881260573864\n",
      "Epoch96 | train loss 0.5684625996463001\n",
      "Epoch97 | train loss 0.5688406208530068\n",
      "Epoch98 | train loss 0.5687948925048113\n",
      "Epoch99 | train loss 0.5686823881231249\n"
     ]
    }
   ],
   "source": [
    "train(model=model,\n",
    "      dataloader=train_dataloader,\n",
    "      loss_fn=loss,\n",
    "      optimizer=optimizer,\n",
    "      epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model(X_test).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_pred , y):\n",
    "    counter = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] != y_pred[i]:\n",
    "            counter +=1\n",
    "    return 1 - counter / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.487"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_fn(y_test_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.state_dict()\n",
    "kernel = params['conv1.0.weight'].view(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6802,  0.5050,  0.6802, -1.5794, -0.5521],\n",
       "        [-0.9250,  1.0620,  1.1711,  0.7767, -2.3699],\n",
       "        [ 0.0088, -4.1072, -2.4593, -1.6428, -1.4181],\n",
       "        [ 0.9703,  1.1122,  0.4479,  0.6916,  0.7387]])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Display the heatmap\n",
    "plt.imshow(kernel, cmap='hot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create more complex conv model \n",
    "class ComplexeConvNet(nn.Module):\n",
    "    # in channels is equal to number of alphabet letters from which dataset is constructed 4 in case of abcd \n",
    "    # 1 out channels = one filter \n",
    "    # let's go with kernel size equal to pattern length 5 in oour case\n",
    "    def __init__(self,num_chars,pattern_len):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(nn.Conv1d(in_channels=num_chars , out_channels = 32 ,kernel_size= pattern_len,bias = True),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(kernel_size=2),\n",
    "                                   nn.Conv1d(in_channels=32 , out_channels = 64 ,kernel_size= pattern_len,bias = True),\n",
    "                                   nn.MaxPool1d(kernel_size=2),\n",
    "                                   nn.Flatten(),\n",
    "                                   nn.Linear(192,64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(in_features=64,out_features=1)\n",
    "                                   )\n",
    "      \n",
    "    def forward(self,x):\n",
    "       \n",
    "        return self.sequential(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ComplexeConvNet(num_chars=4,pattern_len=5)\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.001)\n",
    "loss2 = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "max_pool1d() Invalid computed output size: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[281], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[270], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, loss_fn, optimizer, epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# loop through dataloader in batches \u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m     17\u001b[0m       \n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m       \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m       y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m       \u001b[38;5;66;03m# calcualte loss \u001b[39;00m\n\u001b[0;32m     23\u001b[0m       loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred,y)\n",
      "File \u001b[1;32mc:\\Users\\akopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[279], line 21\u001b[0m, in \u001b[0;36mComplexeConvNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\akopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\akopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\akopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:134\u001b[0m, in \u001b[0;36mMaxPool1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:740\u001b[0m, in \u001b[0;36m_max_pool1d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    739\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: max_pool1d() Invalid computed output size: 0"
     ]
    }
   ],
   "source": [
    "train(model=model2,\n",
    "      dataloader=train_dataloader,\n",
    "      loss_fn=loss2,\n",
    "      optimizer=optimizer2,\n",
    "      epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
