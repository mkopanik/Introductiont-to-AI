{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1\n",
    "\n",
    "1. Generate strings of length 15 over the alphabet a, b, c, d\n",
    "\n",
    "2. Label your strings basing on matching a 5-element regular expression\n",
    "\n",
    "3. Balance your dataset of size 10000 so that approximately half of the dataset contains regex-matching parts.\n",
    "\n",
    "4. Prepare your data for training using one-hot encoding\n",
    "\n",
    "5. Divide your dataset into training and testing parts.\n",
    "\n",
    "6. Implement and train a model consisting of one convolutional layer with one filter followed by one fully-connected layer and train it to classify your strings. After training, examine the values of the filter\n",
    "\n",
    "7. Implement and train more complex models (more filters, layers) and analyze their performance on the prepared dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "def generate_string(length = 15, alphabet = \"abcd\", pattern = \"cabad\", contains = True):\n",
    "      \n",
    "      pattern_len = len(pattern)\n",
    "      if contains: \n",
    "            # we need to generate prefix and sufix to our pattern both might be 0 lentgth ofc \n",
    "            prefix_len = random.randint(0,length - pattern_len)\n",
    "            suffix_len = length - pattern_len - prefix_len\n",
    "            prefix = ''.join(random.choices(\"abcd\" , k=prefix_len))\n",
    "            suffix = ''.join(random.choices(\"abcd\" , k=suffix_len))\n",
    "            return prefix + pattern + suffix\n",
    "      else: \n",
    "            while True:\n",
    "                  candidate = ''.join(random.choices(\"abcd\" , k=length))\n",
    "                  if pattern not in candidate:\n",
    "                        return candidate\n",
    "def generate_dataset(size = 10000):\n",
    "      expressions = []\n",
    "      labels = []\n",
    "      half = size // 2\n",
    "      \n",
    "      \n",
    "      for _ in range(half):\n",
    "            expressions.append(generate_string(length=15,alphabet=\"abcd\",pattern=\"cabad\",contains=True))\n",
    "            labels.append(1)\n",
    "      for _ in range(size- half):\n",
    "            expressions.append(generate_string(length=15,alphabet=\"abcd\",pattern=\"cabad\",contains=False))\n",
    "            labels.append(0)\n",
    "            \n",
    "      return pd.DataFrame({\"expressions\":expressions,\"contains_abcd\":labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cccdadcdcaabdcb'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_string(contains=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expressions</th>\n",
       "      <th>contains_abcd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baaddcabadbabda</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aacabadabcdbbdc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dddcabadcadbbaa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dcabadbcbcbcdcc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cddbccabadcdadc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>ddbdbaadbbabada</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>cccacccdbcacccb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>dcbaddbdbdbdbaa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>dbdcbcaaabaaadd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>acbcadddcdbdbcd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          expressions  contains_abcd\n",
       "0     baaddcabadbabda              1\n",
       "1     aacabadabcdbbdc              1\n",
       "2     dddcabadcadbbaa              1\n",
       "3     dcabadbcbcbcdcc              1\n",
       "4     cddbccabadcdadc              1\n",
       "...               ...            ...\n",
       "9995  ddbdbaadbbabada              0\n",
       "9996  cccacccdbcacccb              0\n",
       "9997  dcbaddbdbdbdbaa              0\n",
       "9998  dbdcbcaaabaaadd              0\n",
       "9999  acbcadddcdbdbcd              0\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indexes = {'a':0,'b':1,'c':2,'d':3}\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_expression(s,map,num_classes):\n",
    "    one_hot = torch.zeros(size = (len(s),num_classes))\n",
    "    for i, char in enumerate(s):\n",
    "        one_hot[i,map[char]] = 1 \n",
    "    return one_hot\n",
    "def one_hot_encode_dataset(dataset,shape,map):\n",
    "    table = torch.empty(size = shape)\n",
    "    for i,s in enumerate(dataset.iloc[:,0]):\n",
    "        table[i] = one_hot_encode_expression(s,map = map,num_classes= shape[-1])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_character(char,characters = ['a','b','c','d']):\n",
    "    index = torch.argmax(char)\n",
    "    return characters[index]\n",
    "\n",
    "def decode_string(dataset, index):\n",
    "    expression = \"\"\n",
    "    for letter in dataset[index]:\n",
    "        expression += decode_character(letter)\n",
    "    return expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = generate_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encode_expression(s,char_indexes,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.empty(size = (10000,15,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_dataset(dataset,shape,map):\n",
    "    table = torch.empty(size = shape)\n",
    "    for i,s in enumerate(dataset.iloc[:,0]):\n",
    "        table[i] = one_hot_encode_expression(s,map = map,num_classes= shape[-1])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_hot_encode_dataset(dataset=df,shape = (10000,15,4),map =char_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_string(dataset=X,index=5000) == df.iloc[5000,0]  # to make sure that one hot encoding works correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(data=df.iloc[:,1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2)  # to have shape as desired by pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create custom dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717us/step - accuracy: 0.5234 - loss: 0.7734 - val_accuracy: 0.5730 - val_loss: 0.6815\n",
      "Epoch 2/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702us/step - accuracy: 0.6045 - loss: 0.6630 - val_accuracy: 0.6170 - val_loss: 0.6541\n",
      "Epoch 3/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697us/step - accuracy: 0.6270 - loss: 0.6402 - val_accuracy: 0.6205 - val_loss: 0.6507\n",
      "Epoch 4/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674us/step - accuracy: 0.6529 - loss: 0.6267 - val_accuracy: 0.6285 - val_loss: 0.6484\n",
      "Epoch 5/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720us/step - accuracy: 0.6400 - loss: 0.6341 - val_accuracy: 0.6295 - val_loss: 0.6472\n",
      "Epoch 6/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 742us/step - accuracy: 0.6487 - loss: 0.6312 - val_accuracy: 0.6335 - val_loss: 0.6471\n",
      "Epoch 7/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666us/step - accuracy: 0.6412 - loss: 0.6342 - val_accuracy: 0.6300 - val_loss: 0.6469\n",
      "Epoch 8/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661us/step - accuracy: 0.6471 - loss: 0.6275 - val_accuracy: 0.6340 - val_loss: 0.6486\n",
      "Epoch 9/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688us/step - accuracy: 0.6507 - loss: 0.6289 - val_accuracy: 0.6390 - val_loss: 0.6436\n",
      "Epoch 10/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 670us/step - accuracy: 0.6533 - loss: 0.6247 - val_accuracy: 0.6360 - val_loss: 0.6430\n",
      "Epoch 11/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 748us/step - accuracy: 0.6586 - loss: 0.6197 - val_accuracy: 0.6365 - val_loss: 0.6445\n",
      "Epoch 12/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726us/step - accuracy: 0.6434 - loss: 0.6306 - val_accuracy: 0.6350 - val_loss: 0.6427\n",
      "Epoch 13/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 735us/step - accuracy: 0.6490 - loss: 0.6280 - val_accuracy: 0.6395 - val_loss: 0.6486\n",
      "Epoch 14/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 680us/step - accuracy: 0.6542 - loss: 0.6212 - val_accuracy: 0.6335 - val_loss: 0.6426\n",
      "Epoch 15/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 677us/step - accuracy: 0.6414 - loss: 0.6266 - val_accuracy: 0.6375 - val_loss: 0.6427\n",
      "Epoch 16/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709us/step - accuracy: 0.6447 - loss: 0.6243 - val_accuracy: 0.6350 - val_loss: 0.6417\n",
      "Epoch 17/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685us/step - accuracy: 0.6377 - loss: 0.6304 - val_accuracy: 0.6355 - val_loss: 0.6498\n",
      "Epoch 18/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 681us/step - accuracy: 0.6565 - loss: 0.6259 - val_accuracy: 0.6370 - val_loss: 0.6409\n",
      "Epoch 19/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 662us/step - accuracy: 0.6397 - loss: 0.6318 - val_accuracy: 0.6390 - val_loss: 0.6431\n",
      "Epoch 20/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686us/step - accuracy: 0.6486 - loss: 0.6231 - val_accuracy: 0.6380 - val_loss: 0.6432\n",
      "Epoch 21/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 671us/step - accuracy: 0.6405 - loss: 0.6284 - val_accuracy: 0.6360 - val_loss: 0.6410\n",
      "Epoch 22/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 672us/step - accuracy: 0.6463 - loss: 0.6303 - val_accuracy: 0.6385 - val_loss: 0.6436\n",
      "Epoch 23/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700us/step - accuracy: 0.6466 - loss: 0.6247 - val_accuracy: 0.6390 - val_loss: 0.6422\n",
      "Epoch 24/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 684us/step - accuracy: 0.6506 - loss: 0.6254 - val_accuracy: 0.6390 - val_loss: 0.6400\n",
      "Epoch 25/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 684us/step - accuracy: 0.6478 - loss: 0.6260 - val_accuracy: 0.6440 - val_loss: 0.6406\n",
      "Epoch 26/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 676us/step - accuracy: 0.6435 - loss: 0.6273 - val_accuracy: 0.6455 - val_loss: 0.6408\n",
      "Epoch 27/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698us/step - accuracy: 0.6562 - loss: 0.6254 - val_accuracy: 0.6390 - val_loss: 0.6438\n",
      "Epoch 28/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727us/step - accuracy: 0.6573 - loss: 0.6240 - val_accuracy: 0.6435 - val_loss: 0.6397\n",
      "Epoch 29/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 736us/step - accuracy: 0.6493 - loss: 0.6238 - val_accuracy: 0.6360 - val_loss: 0.6397\n",
      "Epoch 30/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 742us/step - accuracy: 0.6416 - loss: 0.6262 - val_accuracy: 0.6390 - val_loss: 0.6401\n",
      "Epoch 31/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698us/step - accuracy: 0.6421 - loss: 0.6263 - val_accuracy: 0.6400 - val_loss: 0.6387\n",
      "Epoch 32/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710us/step - accuracy: 0.6445 - loss: 0.6247 - val_accuracy: 0.6435 - val_loss: 0.6434\n",
      "Epoch 33/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 728us/step - accuracy: 0.6437 - loss: 0.6304 - val_accuracy: 0.6430 - val_loss: 0.6391\n",
      "Epoch 34/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711us/step - accuracy: 0.6480 - loss: 0.6244 - val_accuracy: 0.6440 - val_loss: 0.6393\n",
      "Epoch 35/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732us/step - accuracy: 0.6456 - loss: 0.6259 - val_accuracy: 0.6405 - val_loss: 0.6405\n",
      "Epoch 36/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744us/step - accuracy: 0.6334 - loss: 0.6316 - val_accuracy: 0.6455 - val_loss: 0.6397\n",
      "Epoch 37/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708us/step - accuracy: 0.6384 - loss: 0.6296 - val_accuracy: 0.6390 - val_loss: 0.6525\n",
      "Epoch 38/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 755us/step - accuracy: 0.6580 - loss: 0.6190 - val_accuracy: 0.6440 - val_loss: 0.6390\n",
      "Epoch 39/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 765us/step - accuracy: 0.6460 - loss: 0.6243 - val_accuracy: 0.6425 - val_loss: 0.6395\n",
      "Epoch 40/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 751us/step - accuracy: 0.6440 - loss: 0.6280 - val_accuracy: 0.6465 - val_loss: 0.6404\n",
      "Epoch 41/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719us/step - accuracy: 0.6448 - loss: 0.6288 - val_accuracy: 0.6440 - val_loss: 0.6388\n",
      "Epoch 42/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 761us/step - accuracy: 0.6521 - loss: 0.6236 - val_accuracy: 0.6375 - val_loss: 0.6388\n",
      "Epoch 43/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 745us/step - accuracy: 0.6383 - loss: 0.6289 - val_accuracy: 0.6470 - val_loss: 0.6391\n",
      "Epoch 44/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 741us/step - accuracy: 0.6443 - loss: 0.6301 - val_accuracy: 0.6390 - val_loss: 0.6383\n",
      "Epoch 45/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 737us/step - accuracy: 0.6475 - loss: 0.6229 - val_accuracy: 0.6405 - val_loss: 0.6417\n",
      "Epoch 46/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719us/step - accuracy: 0.6454 - loss: 0.6280 - val_accuracy: 0.6445 - val_loss: 0.6432\n",
      "Epoch 47/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708us/step - accuracy: 0.6440 - loss: 0.6267 - val_accuracy: 0.6405 - val_loss: 0.6437\n",
      "Epoch 48/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710us/step - accuracy: 0.6449 - loss: 0.6259 - val_accuracy: 0.6455 - val_loss: 0.6380\n",
      "Epoch 49/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707us/step - accuracy: 0.6496 - loss: 0.6264 - val_accuracy: 0.6460 - val_loss: 0.6386\n",
      "Epoch 50/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744us/step - accuracy: 0.6412 - loss: 0.6266 - val_accuracy: 0.6465 - val_loss: 0.6396\n",
      "Epoch 51/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722us/step - accuracy: 0.6509 - loss: 0.6264 - val_accuracy: 0.6445 - val_loss: 0.6384\n",
      "Epoch 52/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711us/step - accuracy: 0.6449 - loss: 0.6278 - val_accuracy: 0.6420 - val_loss: 0.6382\n",
      "Epoch 53/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722us/step - accuracy: 0.6482 - loss: 0.6289 - val_accuracy: 0.6355 - val_loss: 0.6383\n",
      "Epoch 54/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713us/step - accuracy: 0.6468 - loss: 0.6208 - val_accuracy: 0.6475 - val_loss: 0.6391\n",
      "Epoch 55/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713us/step - accuracy: 0.6373 - loss: 0.6271 - val_accuracy: 0.6470 - val_loss: 0.6403\n",
      "Epoch 56/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717us/step - accuracy: 0.6466 - loss: 0.6236 - val_accuracy: 0.6470 - val_loss: 0.6380\n",
      "Epoch 57/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717us/step - accuracy: 0.6538 - loss: 0.6215 - val_accuracy: 0.6490 - val_loss: 0.6394\n",
      "Epoch 58/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712us/step - accuracy: 0.6564 - loss: 0.6190 - val_accuracy: 0.6475 - val_loss: 0.6387\n",
      "Epoch 59/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720us/step - accuracy: 0.6509 - loss: 0.6227 - val_accuracy: 0.6465 - val_loss: 0.6380\n",
      "Epoch 60/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721us/step - accuracy: 0.6423 - loss: 0.6261 - val_accuracy: 0.6465 - val_loss: 0.6389\n",
      "Epoch 61/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727us/step - accuracy: 0.6471 - loss: 0.6318 - val_accuracy: 0.6445 - val_loss: 0.6414\n",
      "Epoch 62/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 731us/step - accuracy: 0.6482 - loss: 0.6281 - val_accuracy: 0.6495 - val_loss: 0.6395\n",
      "Epoch 63/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725us/step - accuracy: 0.6437 - loss: 0.6292 - val_accuracy: 0.6470 - val_loss: 0.6391\n",
      "Epoch 64/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718us/step - accuracy: 0.6443 - loss: 0.6259 - val_accuracy: 0.6485 - val_loss: 0.6383\n",
      "Epoch 65/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726us/step - accuracy: 0.6487 - loss: 0.6250 - val_accuracy: 0.6445 - val_loss: 0.6417\n",
      "Epoch 66/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721us/step - accuracy: 0.6508 - loss: 0.6212 - val_accuracy: 0.6480 - val_loss: 0.6379\n",
      "Epoch 67/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716us/step - accuracy: 0.6544 - loss: 0.6173 - val_accuracy: 0.6380 - val_loss: 0.6383\n",
      "Epoch 68/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711us/step - accuracy: 0.6466 - loss: 0.6276 - val_accuracy: 0.6505 - val_loss: 0.6381\n",
      "Epoch 69/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721us/step - accuracy: 0.6517 - loss: 0.6275 - val_accuracy: 0.6505 - val_loss: 0.6393\n",
      "Epoch 70/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725us/step - accuracy: 0.6407 - loss: 0.6294 - val_accuracy: 0.6475 - val_loss: 0.6420\n",
      "Epoch 71/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717us/step - accuracy: 0.6524 - loss: 0.6209 - val_accuracy: 0.6490 - val_loss: 0.6406\n",
      "Epoch 72/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 731us/step - accuracy: 0.6458 - loss: 0.6250 - val_accuracy: 0.6505 - val_loss: 0.6378\n",
      "Epoch 73/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732us/step - accuracy: 0.6426 - loss: 0.6253 - val_accuracy: 0.6430 - val_loss: 0.6375\n",
      "Epoch 74/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 741us/step - accuracy: 0.6611 - loss: 0.6157 - val_accuracy: 0.6380 - val_loss: 0.6378\n",
      "Epoch 75/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 760us/step - accuracy: 0.6468 - loss: 0.6237 - val_accuracy: 0.6535 - val_loss: 0.6380\n",
      "Epoch 76/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 810us/step - accuracy: 0.6576 - loss: 0.6193 - val_accuracy: 0.6510 - val_loss: 0.6378\n",
      "Epoch 77/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 728us/step - accuracy: 0.6535 - loss: 0.6229 - val_accuracy: 0.6500 - val_loss: 0.6389\n",
      "Epoch 78/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721us/step - accuracy: 0.6326 - loss: 0.6303 - val_accuracy: 0.6495 - val_loss: 0.6404\n",
      "Epoch 79/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724us/step - accuracy: 0.6337 - loss: 0.6263 - val_accuracy: 0.6495 - val_loss: 0.6381\n",
      "Epoch 80/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 734us/step - accuracy: 0.6432 - loss: 0.6265 - val_accuracy: 0.6460 - val_loss: 0.6409\n",
      "Epoch 81/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 735us/step - accuracy: 0.6499 - loss: 0.6221 - val_accuracy: 0.6510 - val_loss: 0.6382\n",
      "Epoch 82/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727us/step - accuracy: 0.6379 - loss: 0.6303 - val_accuracy: 0.6495 - val_loss: 0.6406\n",
      "Epoch 83/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 728us/step - accuracy: 0.6484 - loss: 0.6261 - val_accuracy: 0.6490 - val_loss: 0.6405\n",
      "Epoch 84/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726us/step - accuracy: 0.6462 - loss: 0.6239 - val_accuracy: 0.6465 - val_loss: 0.6379\n",
      "Epoch 85/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724us/step - accuracy: 0.6443 - loss: 0.6218 - val_accuracy: 0.6440 - val_loss: 0.6376\n",
      "Epoch 86/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 728us/step - accuracy: 0.6420 - loss: 0.6245 - val_accuracy: 0.6535 - val_loss: 0.6396\n",
      "Epoch 87/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 728us/step - accuracy: 0.6511 - loss: 0.6232 - val_accuracy: 0.6515 - val_loss: 0.6389\n",
      "Epoch 88/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726us/step - accuracy: 0.6417 - loss: 0.6280 - val_accuracy: 0.6465 - val_loss: 0.6415\n",
      "Epoch 89/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725us/step - accuracy: 0.6453 - loss: 0.6281 - val_accuracy: 0.6490 - val_loss: 0.6417\n",
      "Epoch 90/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729us/step - accuracy: 0.6319 - loss: 0.6331 - val_accuracy: 0.6470 - val_loss: 0.6384\n",
      "Epoch 91/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732us/step - accuracy: 0.6505 - loss: 0.6262 - val_accuracy: 0.6515 - val_loss: 0.6401\n",
      "Epoch 92/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727us/step - accuracy: 0.6591 - loss: 0.6211 - val_accuracy: 0.6515 - val_loss: 0.6374\n",
      "Epoch 93/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721us/step - accuracy: 0.6532 - loss: 0.6255 - val_accuracy: 0.6485 - val_loss: 0.6389\n",
      "Epoch 94/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721us/step - accuracy: 0.6519 - loss: 0.6194 - val_accuracy: 0.6515 - val_loss: 0.6382\n",
      "Epoch 95/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723us/step - accuracy: 0.6407 - loss: 0.6230 - val_accuracy: 0.6495 - val_loss: 0.6390\n",
      "Epoch 96/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 731us/step - accuracy: 0.6529 - loss: 0.6220 - val_accuracy: 0.6480 - val_loss: 0.6398\n",
      "Epoch 97/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 756us/step - accuracy: 0.6418 - loss: 0.6319 - val_accuracy: 0.6505 - val_loss: 0.6386\n",
      "Epoch 98/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732us/step - accuracy: 0.6392 - loss: 0.6267 - val_accuracy: 0.6495 - val_loss: 0.6384\n",
      "Epoch 99/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722us/step - accuracy: 0.6543 - loss: 0.6213 - val_accuracy: 0.6465 - val_loss: 0.6378\n",
      "Epoch 100/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 794us/step - accuracy: 0.6498 - loss: 0.6235 - val_accuracy: 0.6485 - val_loss: 0.6418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ec4efd3110>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "\n",
    "simple_model = models.Sequential([\n",
    "    layers.Input((15, 4)),\n",
    "    layers.Conv1D(filters = 1, kernel_size=5,use_bias = False),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1, activation='sigmoid',use_bias = False)\n",
    "])\n",
    "\n",
    "simple_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "simple_model.fit(X_train, y_train, batch_size= 10,epochs = 100, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_pred , y):\n",
    "    y_pred = np.array(y_pred)\n",
    "    y = np.array(y)\n",
    "    counter = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] != y_pred[i]:\n",
    "            counter +=1\n",
    "    return 1 - counter / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.round(simple_model(X_test)).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6485000000000001"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_fn(y_test_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Access the Conv1D layer\n",
    "conv_layer = simple_model.layers[0]\n",
    "\n",
    "# Extract the weights (kernel and bias)\n",
    "kernel = conv_layer.get_weights()\n",
    "kernel = np.array(kernel).reshape(4, 5)   # 5 element regex\n",
    "\n",
    "plt.imshow(kernel, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7477 - loss: 0.4791 - val_accuracy: 0.9840 - val_loss: 0.0619\n",
      "Epoch 2/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9878 - loss: 0.0526 - val_accuracy: 0.9960 - val_loss: 0.0168\n",
      "Epoch 3/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9966 - loss: 0.0173 - val_accuracy: 0.9980 - val_loss: 0.0099\n",
      "Epoch 4/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9971 - loss: 0.0135 - val_accuracy: 0.9985 - val_loss: 0.0092\n",
      "Epoch 5/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9990 - loss: 0.0063 - val_accuracy: 0.9985 - val_loss: 0.0140\n",
      "Epoch 6/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9978 - loss: 0.0085 - val_accuracy: 0.9980 - val_loss: 0.0093\n",
      "Epoch 7/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.0052 - val_accuracy: 0.9985 - val_loss: 0.0129\n",
      "Epoch 8/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 0.9980 - val_loss: 0.0083\n",
      "Epoch 9/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9991 - loss: 0.0045 - val_accuracy: 0.9985 - val_loss: 0.0105\n",
      "Epoch 10/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9988 - loss: 0.0037 - val_accuracy: 0.9980 - val_loss: 0.0100\n",
      "Epoch 11/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9987 - loss: 0.0045 - val_accuracy: 0.9980 - val_loss: 0.0099\n",
      "Epoch 12/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9995 - loss: 0.0021 - val_accuracy: 0.9985 - val_loss: 0.0106\n",
      "Epoch 13/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9979 - loss: 0.0059 - val_accuracy: 0.9980 - val_loss: 0.0107\n",
      "Epoch 14/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9999 - loss: 0.0017 - val_accuracy: 0.9975 - val_loss: 0.0117\n",
      "Epoch 15/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9993 - loss: 0.0018 - val_accuracy: 0.9980 - val_loss: 0.0137\n",
      "Epoch 16/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 5.4366e-04 - val_accuracy: 0.9975 - val_loss: 0.0179\n",
      "Epoch 17/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9975 - loss: 0.0099 - val_accuracy: 0.9980 - val_loss: 0.0124\n",
      "Epoch 18/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 8.1340e-04 - val_accuracy: 0.9980 - val_loss: 0.0130\n",
      "Epoch 19/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 3.2501e-04 - val_accuracy: 0.9980 - val_loss: 0.0137\n",
      "Epoch 20/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 3.1079e-04 - val_accuracy: 0.9975 - val_loss: 0.0139\n",
      "Epoch 21/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.9617e-04 - val_accuracy: 0.9980 - val_loss: 0.0147\n",
      "Epoch 22/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.0367e-04 - val_accuracy: 0.9980 - val_loss: 0.0149\n",
      "Epoch 23/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 8.9141e-05 - val_accuracy: 0.9980 - val_loss: 0.0154\n",
      "Epoch 24/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 4.8811e-05 - val_accuracy: 0.9980 - val_loss: 0.0159\n",
      "Epoch 25/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 4.4047e-05 - val_accuracy: 0.9980 - val_loss: 0.0164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ec5842cb00>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv1D(filters = 32, kernel_size=3, activation='relu', input_shape=(15, 4)),\n",
    "    layers.MaxPool1D(pool_size = 2),\n",
    "    layers.Conv1D(64, 3, activation = 'relu'), \n",
    "    layers.MaxPool1D(2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation = 'relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 25, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.round(model(X_test)).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_fn(y_test_pred,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
