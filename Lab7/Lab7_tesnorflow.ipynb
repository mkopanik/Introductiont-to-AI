{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1\n",
    "\n",
    "1. Generate strings of length 15 over the alphabet a, b, c, d\n",
    "\n",
    "2. Label your strings basing on matching a 5-element regular expression\n",
    "\n",
    "3. Balance your dataset of size 10000 so that approximately half of the dataset contains regex-matching parts.\n",
    "\n",
    "4. Prepare your data for training using one-hot encoding\n",
    "\n",
    "5. Divide your dataset into training and testing parts.\n",
    "\n",
    "6. Implement and train a model consisting of one convolutional layer with one filter followed by one fully-connected layer and train it to classify your strings. After training, examine the values of the filter\n",
    "\n",
    "7. Implement and train more complex models (more filters, layers) and analyze their performance on the prepared dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "def generate_string(length = 15, alphabet = \"abcd\", pattern = \"cabad\", contains = True):\n",
    "      \n",
    "      pattern_len = len(pattern)\n",
    "      if contains: \n",
    "            # we need to generate prefix and sufix to our pattern both might be 0 lentgth ofc \n",
    "            prefix_len = random.randint(0,length - pattern_len)\n",
    "            suffix_len = length - pattern_len - prefix_len\n",
    "            prefix = ''.join(random.choices(\"abcd\" , k=prefix_len))\n",
    "            suffix = ''.join(random.choices(\"abcd\" , k=suffix_len))\n",
    "            return prefix + pattern + suffix\n",
    "      else: \n",
    "            while True:\n",
    "                  candidate = ''.join(random.choices(\"abcd\" , k=length))\n",
    "                  if pattern not in candidate:\n",
    "                        return candidate\n",
    "def generate_dataset(size = 10000):\n",
    "      expressions = []\n",
    "      labels = []\n",
    "      # half = size // 2\n",
    "      half = 9000\n",
    "      \n",
    "      \n",
    "      for _ in range(half):\n",
    "            expressions.append(generate_string(length=15,alphabet=\"abcd\",pattern=\"cabad\",contains=True))\n",
    "            labels.append(1)\n",
    "      for _ in range(size- half):\n",
    "            expressions.append(generate_string(length=15,alphabet=\"abcd\",pattern=\"cabad\",contains=False))\n",
    "            labels.append(0)\n",
    "            \n",
    "      return pd.DataFrame({\"expressions\":expressions,\"contains_pattern\":labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ddcbccbbbbddadc'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_string(contains=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bacbcdcabadccaa'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_string(contains=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indexes = {'a':0,'b':1,'c':2,'d':3}\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_expression(s,map,num_classes):\n",
    "    one_hot = torch.zeros(size = (len(s),num_classes))\n",
    "    for i, char in enumerate(s):\n",
    "        one_hot[i,map[char]] = 1 \n",
    "    return one_hot\n",
    "def one_hot_encode_dataset(dataset,shape,map):\n",
    "    table = torch.empty(size = shape)\n",
    "    for i,s in enumerate(dataset.iloc[:,0]):\n",
    "        table[i] = one_hot_encode_expression(s,map = map,num_classes= shape[-1])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_character(char,characters = ['a','b','c','d']):\n",
    "    index = torch.argmax(char)\n",
    "    return characters[index]\n",
    "\n",
    "def decode_string(dataset, index):\n",
    "    expression = \"\"\n",
    "    for letter in dataset[index]:\n",
    "        expression += decode_character(letter)\n",
    "    return expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = generate_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encode_expression(s,char_indexes,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.empty(size = (10000,15,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_dataset(dataset,shape,map):\n",
    "    table = torch.empty(size = shape)\n",
    "    for i,s in enumerate(dataset.iloc[:,0]):\n",
    "        table[i] = one_hot_encode_expression(s,map = map,num_classes= shape[-1])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_hot_encode_dataset(dataset=df,shape = (10000,15,4),map =char_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_string(dataset=X,index=5000) == df.iloc[5000,0]  # to make sure that one hot encoding works correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(data=df.iloc[:,1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2)  # to have shape as desired by pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create custom dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 834us/step - accuracy: 0.7729 - loss: 0.4788 - val_accuracy: 0.8955 - val_loss: 0.3278\n",
      "Epoch 2/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692us/step - accuracy: 0.9032 - loss: 0.3124 - val_accuracy: 0.8955 - val_loss: 0.3218\n",
      "Epoch 3/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668us/step - accuracy: 0.8926 - loss: 0.3309 - val_accuracy: 0.8955 - val_loss: 0.3194\n",
      "Epoch 4/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 680us/step - accuracy: 0.9016 - loss: 0.3128 - val_accuracy: 0.8955 - val_loss: 0.3173\n",
      "Epoch 5/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 670us/step - accuracy: 0.8987 - loss: 0.3165 - val_accuracy: 0.8955 - val_loss: 0.3142\n",
      "Epoch 6/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674us/step - accuracy: 0.8966 - loss: 0.3173 - val_accuracy: 0.8955 - val_loss: 0.3116\n",
      "Epoch 7/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666us/step - accuracy: 0.9032 - loss: 0.3020 - val_accuracy: 0.8955 - val_loss: 0.3102\n",
      "Epoch 8/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 667us/step - accuracy: 0.8981 - loss: 0.3099 - val_accuracy: 0.8955 - val_loss: 0.3084\n",
      "Epoch 9/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 662us/step - accuracy: 0.9016 - loss: 0.3025 - val_accuracy: 0.8955 - val_loss: 0.3069\n",
      "Epoch 10/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708us/step - accuracy: 0.8996 - loss: 0.3096 - val_accuracy: 0.8955 - val_loss: 0.3056\n",
      "Epoch 11/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 678us/step - accuracy: 0.9020 - loss: 0.3060 - val_accuracy: 0.8955 - val_loss: 0.3043\n",
      "Epoch 12/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709us/step - accuracy: 0.8994 - loss: 0.3074 - val_accuracy: 0.8955 - val_loss: 0.3035\n",
      "Epoch 13/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666us/step - accuracy: 0.9018 - loss: 0.3023 - val_accuracy: 0.8955 - val_loss: 0.3043\n",
      "Epoch 14/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674us/step - accuracy: 0.9049 - loss: 0.2944 - val_accuracy: 0.8955 - val_loss: 0.3025\n",
      "Epoch 15/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699us/step - accuracy: 0.9025 - loss: 0.2968 - val_accuracy: 0.8960 - val_loss: 0.3016\n",
      "Epoch 16/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 673us/step - accuracy: 0.9057 - loss: 0.2954 - val_accuracy: 0.8965 - val_loss: 0.3004\n",
      "Epoch 17/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685us/step - accuracy: 0.9022 - loss: 0.3009 - val_accuracy: 0.8965 - val_loss: 0.3000\n",
      "Epoch 18/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 677us/step - accuracy: 0.8979 - loss: 0.3072 - val_accuracy: 0.8965 - val_loss: 0.2998\n",
      "Epoch 19/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 680us/step - accuracy: 0.9001 - loss: 0.3066 - val_accuracy: 0.8970 - val_loss: 0.2993\n",
      "Epoch 20/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 664us/step - accuracy: 0.9039 - loss: 0.2986 - val_accuracy: 0.8965 - val_loss: 0.2988\n",
      "Epoch 21/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 667us/step - accuracy: 0.8979 - loss: 0.3079 - val_accuracy: 0.8965 - val_loss: 0.2991\n",
      "Epoch 22/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 650us/step - accuracy: 0.9052 - loss: 0.2946 - val_accuracy: 0.8965 - val_loss: 0.2981\n",
      "Epoch 23/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657us/step - accuracy: 0.9038 - loss: 0.2967 - val_accuracy: 0.8965 - val_loss: 0.2980\n",
      "Epoch 24/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 663us/step - accuracy: 0.8995 - loss: 0.3009 - val_accuracy: 0.8975 - val_loss: 0.2979\n",
      "Epoch 25/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674us/step - accuracy: 0.9021 - loss: 0.2947 - val_accuracy: 0.8965 - val_loss: 0.2972\n",
      "Epoch 26/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 679us/step - accuracy: 0.8994 - loss: 0.3094 - val_accuracy: 0.8980 - val_loss: 0.2970\n",
      "Epoch 27/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 664us/step - accuracy: 0.9045 - loss: 0.2967 - val_accuracy: 0.8965 - val_loss: 0.3010\n",
      "Epoch 28/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 672us/step - accuracy: 0.9033 - loss: 0.2969 - val_accuracy: 0.8975 - val_loss: 0.2961\n",
      "Epoch 29/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 677us/step - accuracy: 0.9037 - loss: 0.2936 - val_accuracy: 0.8970 - val_loss: 0.2960\n",
      "Epoch 30/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 673us/step - accuracy: 0.8977 - loss: 0.3120 - val_accuracy: 0.8970 - val_loss: 0.2959\n",
      "Epoch 31/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 669us/step - accuracy: 0.9014 - loss: 0.3012 - val_accuracy: 0.8975 - val_loss: 0.2951\n",
      "Epoch 32/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 672us/step - accuracy: 0.8972 - loss: 0.3095 - val_accuracy: 0.8975 - val_loss: 0.2950\n",
      "Epoch 33/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 670us/step - accuracy: 0.8974 - loss: 0.3042 - val_accuracy: 0.8970 - val_loss: 0.2955\n",
      "Epoch 34/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686us/step - accuracy: 0.9009 - loss: 0.2999 - val_accuracy: 0.8970 - val_loss: 0.2952\n",
      "Epoch 35/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 681us/step - accuracy: 0.8984 - loss: 0.3043 - val_accuracy: 0.8970 - val_loss: 0.2955\n",
      "Epoch 36/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 652us/step - accuracy: 0.9048 - loss: 0.2942 - val_accuracy: 0.8975 - val_loss: 0.2942\n",
      "Epoch 37/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702us/step - accuracy: 0.8985 - loss: 0.3050 - val_accuracy: 0.8975 - val_loss: 0.2942\n",
      "Epoch 38/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706us/step - accuracy: 0.9051 - loss: 0.2957 - val_accuracy: 0.8975 - val_loss: 0.2939\n",
      "Epoch 39/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 737us/step - accuracy: 0.8980 - loss: 0.3053 - val_accuracy: 0.8970 - val_loss: 0.2954\n",
      "Epoch 40/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 663us/step - accuracy: 0.9018 - loss: 0.2980 - val_accuracy: 0.8975 - val_loss: 0.2939\n",
      "Epoch 41/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666us/step - accuracy: 0.9024 - loss: 0.2994 - val_accuracy: 0.8975 - val_loss: 0.2935\n",
      "Epoch 42/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668us/step - accuracy: 0.9054 - loss: 0.2883 - val_accuracy: 0.8980 - val_loss: 0.2938\n",
      "Epoch 43/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 738us/step - accuracy: 0.9016 - loss: 0.2981 - val_accuracy: 0.8965 - val_loss: 0.2951\n",
      "Epoch 44/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 680us/step - accuracy: 0.9085 - loss: 0.2880 - val_accuracy: 0.8975 - val_loss: 0.2934\n",
      "Epoch 45/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 672us/step - accuracy: 0.9038 - loss: 0.2940 - val_accuracy: 0.8980 - val_loss: 0.2927\n",
      "Epoch 46/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 682us/step - accuracy: 0.9032 - loss: 0.2960 - val_accuracy: 0.8980 - val_loss: 0.2926\n",
      "Epoch 47/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 675us/step - accuracy: 0.9019 - loss: 0.3004 - val_accuracy: 0.8975 - val_loss: 0.2928\n",
      "Epoch 48/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696us/step - accuracy: 0.9046 - loss: 0.2886 - val_accuracy: 0.8975 - val_loss: 0.2930\n",
      "Epoch 49/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712us/step - accuracy: 0.9019 - loss: 0.2964 - val_accuracy: 0.8980 - val_loss: 0.2924\n",
      "Epoch 50/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685us/step - accuracy: 0.9028 - loss: 0.2961 - val_accuracy: 0.8975 - val_loss: 0.2927\n",
      "Epoch 51/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 667us/step - accuracy: 0.9079 - loss: 0.2871 - val_accuracy: 0.8980 - val_loss: 0.2925\n",
      "Epoch 52/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700us/step - accuracy: 0.8990 - loss: 0.3028 - val_accuracy: 0.8965 - val_loss: 0.2951\n",
      "Epoch 53/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 679us/step - accuracy: 0.9057 - loss: 0.2934 - val_accuracy: 0.8995 - val_loss: 0.2925\n",
      "Epoch 54/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 679us/step - accuracy: 0.9033 - loss: 0.2939 - val_accuracy: 0.8970 - val_loss: 0.2934\n",
      "Epoch 55/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661us/step - accuracy: 0.9032 - loss: 0.2940 - val_accuracy: 0.8975 - val_loss: 0.2920\n",
      "Epoch 56/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 662us/step - accuracy: 0.8998 - loss: 0.3022 - val_accuracy: 0.8970 - val_loss: 0.2938\n",
      "Epoch 57/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 687us/step - accuracy: 0.9023 - loss: 0.3003 - val_accuracy: 0.8975 - val_loss: 0.2921\n",
      "Epoch 58/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707us/step - accuracy: 0.8985 - loss: 0.3072 - val_accuracy: 0.8985 - val_loss: 0.2914\n",
      "Epoch 59/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 672us/step - accuracy: 0.9019 - loss: 0.2945 - val_accuracy: 0.8975 - val_loss: 0.2917\n",
      "Epoch 60/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716us/step - accuracy: 0.9043 - loss: 0.2963 - val_accuracy: 0.8980 - val_loss: 0.2912\n",
      "Epoch 61/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 675us/step - accuracy: 0.9053 - loss: 0.2870 - val_accuracy: 0.9000 - val_loss: 0.2920\n",
      "Epoch 62/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718us/step - accuracy: 0.9008 - loss: 0.3028 - val_accuracy: 0.8985 - val_loss: 0.2916\n",
      "Epoch 63/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719us/step - accuracy: 0.9005 - loss: 0.3044 - val_accuracy: 0.8985 - val_loss: 0.2912\n",
      "Epoch 64/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 684us/step - accuracy: 0.9085 - loss: 0.2834 - val_accuracy: 0.8970 - val_loss: 0.2925\n",
      "Epoch 65/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 664us/step - accuracy: 0.8978 - loss: 0.3105 - val_accuracy: 0.8970 - val_loss: 0.2927\n",
      "Epoch 66/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 658us/step - accuracy: 0.9085 - loss: 0.2856 - val_accuracy: 0.8985 - val_loss: 0.2909\n",
      "Epoch 67/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 654us/step - accuracy: 0.9036 - loss: 0.2981 - val_accuracy: 0.8985 - val_loss: 0.2909\n",
      "Epoch 68/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715us/step - accuracy: 0.9000 - loss: 0.3014 - val_accuracy: 0.8985 - val_loss: 0.2911\n",
      "Epoch 69/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723us/step - accuracy: 0.9027 - loss: 0.2948 - val_accuracy: 0.8980 - val_loss: 0.2912\n",
      "Epoch 70/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718us/step - accuracy: 0.9022 - loss: 0.2963 - val_accuracy: 0.8975 - val_loss: 0.2918\n",
      "Epoch 71/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714us/step - accuracy: 0.9097 - loss: 0.2792 - val_accuracy: 0.8985 - val_loss: 0.2908\n",
      "Epoch 72/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706us/step - accuracy: 0.9015 - loss: 0.2982 - val_accuracy: 0.8985 - val_loss: 0.2910\n",
      "Epoch 73/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 676us/step - accuracy: 0.9013 - loss: 0.2990 - val_accuracy: 0.8985 - val_loss: 0.2908\n",
      "Epoch 74/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 683us/step - accuracy: 0.9006 - loss: 0.3005 - val_accuracy: 0.8975 - val_loss: 0.2918\n",
      "Epoch 75/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717us/step - accuracy: 0.9037 - loss: 0.2912 - val_accuracy: 0.8980 - val_loss: 0.2911\n",
      "Epoch 76/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700us/step - accuracy: 0.9039 - loss: 0.2929 - val_accuracy: 0.8980 - val_loss: 0.2913\n",
      "Epoch 77/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716us/step - accuracy: 0.9046 - loss: 0.2912 - val_accuracy: 0.8980 - val_loss: 0.2912\n",
      "Epoch 78/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690us/step - accuracy: 0.9089 - loss: 0.2840 - val_accuracy: 0.8980 - val_loss: 0.2911\n",
      "Epoch 79/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690us/step - accuracy: 0.8947 - loss: 0.3124 - val_accuracy: 0.8995 - val_loss: 0.2914\n",
      "Epoch 80/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704us/step - accuracy: 0.9093 - loss: 0.2830 - val_accuracy: 0.8980 - val_loss: 0.2912\n",
      "Epoch 81/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696us/step - accuracy: 0.9016 - loss: 0.3014 - val_accuracy: 0.9015 - val_loss: 0.2908\n",
      "Epoch 82/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719us/step - accuracy: 0.9049 - loss: 0.2924 - val_accuracy: 0.8975 - val_loss: 0.2917\n",
      "Epoch 83/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 687us/step - accuracy: 0.9048 - loss: 0.2954 - val_accuracy: 0.8980 - val_loss: 0.2912\n",
      "Epoch 84/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 681us/step - accuracy: 0.9047 - loss: 0.2884 - val_accuracy: 0.8995 - val_loss: 0.2913\n",
      "Epoch 85/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691us/step - accuracy: 0.9041 - loss: 0.2973 - val_accuracy: 0.8975 - val_loss: 0.2927\n",
      "Epoch 86/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701us/step - accuracy: 0.9027 - loss: 0.2934 - val_accuracy: 0.8975 - val_loss: 0.2922\n",
      "Epoch 87/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689us/step - accuracy: 0.8980 - loss: 0.3027 - val_accuracy: 0.8985 - val_loss: 0.2911\n",
      "Epoch 88/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 679us/step - accuracy: 0.9020 - loss: 0.3006 - val_accuracy: 0.8975 - val_loss: 0.2925\n",
      "Epoch 89/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 682us/step - accuracy: 0.9074 - loss: 0.2819 - val_accuracy: 0.8985 - val_loss: 0.2913\n",
      "Epoch 90/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 670us/step - accuracy: 0.9053 - loss: 0.2918 - val_accuracy: 0.8990 - val_loss: 0.2912\n",
      "Epoch 91/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685us/step - accuracy: 0.9058 - loss: 0.2888 - val_accuracy: 0.8980 - val_loss: 0.2916\n",
      "Epoch 92/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666us/step - accuracy: 0.9063 - loss: 0.2880 - val_accuracy: 0.9035 - val_loss: 0.2940\n",
      "Epoch 93/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688us/step - accuracy: 0.8966 - loss: 0.3060 - val_accuracy: 0.8980 - val_loss: 0.2915\n",
      "Epoch 94/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 681us/step - accuracy: 0.8999 - loss: 0.3075 - val_accuracy: 0.8975 - val_loss: 0.2943\n",
      "Epoch 95/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 679us/step - accuracy: 0.9035 - loss: 0.2930 - val_accuracy: 0.8985 - val_loss: 0.2916\n",
      "Epoch 96/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 678us/step - accuracy: 0.9081 - loss: 0.2857 - val_accuracy: 0.9015 - val_loss: 0.2925\n",
      "Epoch 97/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697us/step - accuracy: 0.9032 - loss: 0.2974 - val_accuracy: 0.8975 - val_loss: 0.2941\n",
      "Epoch 98/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 673us/step - accuracy: 0.9049 - loss: 0.2872 - val_accuracy: 0.8995 - val_loss: 0.2918\n",
      "Epoch 99/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697us/step - accuracy: 0.9033 - loss: 0.2999 - val_accuracy: 0.8990 - val_loss: 0.2915\n",
      "Epoch 100/100\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700us/step - accuracy: 0.9008 - loss: 0.2973 - val_accuracy: 0.8980 - val_loss: 0.2918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c7d0853200>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "\n",
    "simple_model = models.Sequential([\n",
    "    layers.Input((15, 4)),\n",
    "    layers.Conv1D(filters = 1, kernel_size=5,use_bias = False),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1, activation='sigmoid',use_bias = False)\n",
    "])\n",
    "\n",
    "simple_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "simple_model.fit(X_train, y_train, batch_size= 10,epochs = 100, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_pred , y):\n",
    "    y_pred = np.array(y_pred)\n",
    "    y = np.array(y)\n",
    "    counter = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] != y_pred[i]:\n",
    "            counter +=1\n",
    "    return 1 - counter / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.round(simple_model(X_test)).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.898"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_fn(y_test_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'cabad'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Access the Conv1D layer\n",
    "conv_layer = simple_model.layers[0]\n",
    "\n",
    "# Extract the weights (kernel and bias)\n",
    "kernel = conv_layer.get_weights()\n",
    "kernel = np.array(kernel).reshape(5, 4)   # 5 element regex\n",
    "\n",
    "sns.heatmap(data = kernel,cmap='hot')\n",
    "plt.show()\n",
    "\"cabad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05643355, -0.06580992,  0.7428065 , -0.1246466 ],\n",
       "       [ 0.78620994, -0.1197167 ,  0.03995285, -0.11408028],\n",
       "       [-0.01428253,  0.45936206, -0.43765062, -0.15570097],\n",
       "       [ 0.5636117 ,  0.05943643, -0.21283199, -0.26421747],\n",
       "       [ 0.13742766, -0.382899  , -0.14220342,  0.7900435 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akopa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8968 - loss: 0.3087 - val_accuracy: 0.9840 - val_loss: 0.0548\n",
      "Epoch 2/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0325 - val_accuracy: 0.9960 - val_loss: 0.0159\n",
      "Epoch 3/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0091 - val_accuracy: 0.9980 - val_loss: 0.0092\n",
      "Epoch 4/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0078 - val_accuracy: 0.9990 - val_loss: 0.0065\n",
      "Epoch 5/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0045 - val_accuracy: 0.9985 - val_loss: 0.0067\n",
      "Epoch 6/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9993 - loss: 0.0036 - val_accuracy: 0.9970 - val_loss: 0.0107\n",
      "Epoch 7/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9987 - loss: 0.0043 - val_accuracy: 0.9975 - val_loss: 0.0093\n",
      "Epoch 8/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.9985 - val_loss: 0.0046\n",
      "Epoch 9/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9994 - loss: 0.0027 - val_accuracy: 0.9970 - val_loss: 0.0120\n",
      "Epoch 10/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0010 - val_accuracy: 0.9985 - val_loss: 0.0063\n",
      "Epoch 11/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9998 - loss: 7.5279e-04 - val_accuracy: 0.9990 - val_loss: 0.0040\n",
      "Epoch 12/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 0.9990 - val_loss: 0.0029\n",
      "Epoch 13/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9995 - loss: 0.0011 - val_accuracy: 0.9990 - val_loss: 0.0025\n",
      "Epoch 14/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 6.1953e-04 - val_accuracy: 0.9985 - val_loss: 0.0049\n",
      "Epoch 15/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 5.6412e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c7defe6240>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv1D(filters = 32, kernel_size=3, activation='relu', input_shape=(15, 4)),\n",
    "    layers.MaxPool1D(pool_size = 2),\n",
    "    layers.Conv1D(64, 3, activation = 'relu'), \n",
    "    layers.MaxPool1D(2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation = 'relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 15, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.round(model(X_test)).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_fn(y_test_pred,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
